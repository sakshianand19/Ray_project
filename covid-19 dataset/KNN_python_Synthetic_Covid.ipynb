{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of KNN on Covid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values  # all columns except the last one as features\n",
    "    y = data.iloc[:, -1].values   # the last column as labels\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and time KNN (without Ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_time_knn(X_train, y_train, X_test, y_test, n_neighbors=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    knn.fit(X_train, y_train)  # Train KNN model\n",
    "    y_pred = knn.predict(X_test)  # Predict on test data\n",
    "    end_time = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return accuracy, time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset_from_csv('usa_county_wise.csv')  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step Preprocessing\n",
    "1. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        UID iso2 iso3  code3     FIPS    Admin2            Province_State   \n",
      "0        16   AS  ASM     16     60.0       NaN            American Samoa  \\\n",
      "1       316   GU  GUM    316     66.0       NaN                      Guam   \n",
      "2       580   MP  MNP    580     69.0       NaN  Northern Mariana Islands   \n",
      "3  63072001   PR  PRI    630  72001.0  Adjuntas               Puerto Rico   \n",
      "4  63072003   PR  PRI    630  72003.0    Aguada               Puerto Rico   \n",
      "\n",
      "  Country_Region        Lat       Long_                  Combined_Key   \n",
      "0             US -14.271000 -170.132000            American Samoa, US  \\\n",
      "1             US  13.444300  144.793700                      Guam, US   \n",
      "2             US  15.097900  145.673900  Northern Mariana Islands, US   \n",
      "3             US  18.180117  -66.754367     Adjuntas, Puerto Rico, US   \n",
      "4             US  18.360255  -67.175131       Aguada, Puerto Rico, US   \n",
      "\n",
      "      Date  Confirmed  Deaths  \n",
      "0  1/22/20          0       0  \n",
      "1  1/22/20          0       0  \n",
      "2  1/22/20          0       0  \n",
      "3  1/22/20          0       0  \n",
      "4  1/22/20          0       0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 627920 entries, 0 to 627919\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   UID             627920 non-null  int64  \n",
      " 1   iso2            627920 non-null  object \n",
      " 2   iso3            627920 non-null  object \n",
      " 3   code3           627920 non-null  int64  \n",
      " 4   FIPS            626040 non-null  float64\n",
      " 5   Admin2          626792 non-null  object \n",
      " 6   Province_State  627920 non-null  object \n",
      " 7   Country_Region  627920 non-null  object \n",
      " 8   Lat             627920 non-null  float64\n",
      " 9   Long_           627920 non-null  float64\n",
      " 10  Combined_Key    627920 non-null  object \n",
      " 11  Date            627920 non-null  object \n",
      " 12  Confirmed       627920 non-null  int64  \n",
      " 13  Deaths          627920 non-null  int64  \n",
      "dtypes: float64(3), int64(4), object(7)\n",
      "memory usage: 67.1+ MB\n",
      "None\n",
      "                UID          code3           FIPS            Lat   \n",
      "count  6.279200e+05  627920.000000  626040.000000  627920.000000  \\\n",
      "mean   8.342958e+07     834.491617   33061.684685      36.707212   \n",
      "std    4.314702e+06      36.492620   18636.156825       9.061572   \n",
      "min    1.600000e+01      16.000000      60.000000     -14.271000   \n",
      "25%    8.401811e+07     840.000000   19079.000000      33.895587   \n",
      "50%    8.402921e+07     840.000000   31014.000000      38.002344   \n",
      "75%    8.404612e+07     840.000000   47131.000000      41.573069   \n",
      "max    8.410000e+07     850.000000   99999.000000      69.314792   \n",
      "\n",
      "               Long_      Confirmed         Deaths  \n",
      "count  627920.000000  627920.000000  627920.000000  \n",
      "mean      -88.601474     357.284285      17.536328  \n",
      "std        21.715747    3487.282694     300.991466  \n",
      "min      -174.159600       0.000000       0.000000  \n",
      "25%       -97.790204       0.000000       0.000000  \n",
      "50%       -89.486710       4.000000       0.000000  \n",
      "75%       -82.311265      63.000000       1.000000  \n",
      "max       145.673900  224051.000000   23500.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('usa_county_wise.csv')\n",
    "print(data.head())  # View the first few rows\n",
    "print(data.info())  # Check column types and missing values\n",
    "print(data.describe())  # Get summary statistics for numeric columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID                  0\n",
      "iso2                 0\n",
      "iso3                 0\n",
      "code3                0\n",
      "FIPS              1880\n",
      "Admin2            1128\n",
      "Province_State       0\n",
      "Country_Region       0\n",
      "Lat                  0\n",
      "Long_                0\n",
      "Combined_Key         0\n",
      "Date                 0\n",
      "Confirmed            0\n",
      "Deaths               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Drop rows with excessive missing values (e.g., Admin2, Province_State)\n",
    "data.dropna(subset=['Admin2', 'Province_State'], inplace=True)\n",
    "\n",
    "# Fill missing numerical values\n",
    "data.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Drop Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Combined_Key', 'Country_Region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Handle Date Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_4132\\1938391907.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Date'] = pd.to_datetime(data['Date'])\n"
     ]
    }
   ],
   "source": [
    "# Convert to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Extract useful date features (optional)\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "\n",
    "# Drop the original 'Date' column if not needed\n",
    "data = data.drop(columns=['Date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Encode Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "data['Admin2'] = encoder.fit_transform(data['Admin2'])\n",
    "data['Province_State'] = encoder.fit_transform(data['Province_State'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Confirmed', 'Deaths'])  # Drop target columns\n",
    "y = data['Confirmed']  # Use 'Deaths' if modeling deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Scale Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Preprocessing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_4132\\2314121145.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Date'] = pd.to_datetime(data['Date'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('usa_county_wise.csv')\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(subset=['Admin2', 'Province_State'], inplace=True)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "data = data.drop(columns=['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Combined_Key', 'Country_Region'])\n",
    "\n",
    "# Handle 'Date' column\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "data = data.drop(columns=['Date'])\n",
    "\n",
    "# Encode categorical columns\n",
    "encoder = LabelEncoder()\n",
    "data['Admin2'] = encoder.fit_transform(data['Admin2'])\n",
    "data['Province_State'] = encoder.fit_transform(data['Province_State'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Confirmed', 'Deaths'])\n",
    "y = data['Confirmed']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (without Ray) -> Accuracy: 51.47%, Time taken: 9.3368 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate KNN\n",
    "accuracy, time_taken = train_and_time_knn(X_train, y_train, X_test, y_test)\n",
    "print(f\"KNN (without Ray) -> Accuracy: {accuracy * 100:.2f}%, Time taken: {time_taken:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
